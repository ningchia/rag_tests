# æ¸¬è©¦å•é¡Œ : éŠ€è¡Œåœ¨èª¿é«˜æŒå¡äººä¿¡ç”¨é¡åº¦å¾Œæœƒç”±ä½•å‹•ä½œï¼Ÿ

import os
import numpy as np
import pandas as pd
import plotly.express as px
from sklearn.decomposition import PCA
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_classic.retrievers import MultiVectorRetriever
from langchain_classic.storage import LocalFileStore
from langchain_core.load import loads as langchain_loads

# 0. å®šç¾©å„²å­˜è·¯å¾‘ (éœ€èˆ‡ä¹‹å‰çš„ç¨‹å¼ä¸€è‡´)
VECTOR_DB_PATH = "./faiss_index_save"
BYTE_STORE_PATH = "./parent_doc_storage_save"
hash_key_name = "doc_hash_id"

def main():
    # 1. åˆå§‹åŒ–çµ„ä»¶èˆ‡è¼‰å…¥ç¾æœ‰è³‡æ–™åº«
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    
    if not os.path.exists(VECTOR_DB_PATH):
        print("éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°å‘é‡åº«è·¯å¾‘ã€‚è«‹å…ˆåŸ·è¡ŒåŒ¯å…¥ç¨‹å¼ã€‚")
        return

    vectorstore = FAISS.load_local(
        VECTOR_DB_PATH, 
        embeddings, 
        allow_dangerous_deserialization=True
    )
    store = LocalFileStore(BYTE_STORE_PATH)

    # å»ºç«‹ Retriever
    retriever = MultiVectorRetriever(
        vectorstore=vectorstore,
        byte_store=store,
        id_key=hash_key_name,
    )

    # å¯ä»¥å‘Šè¨´æª¢ç´¢å™¨ï¼šæ¯æ¬¡æœå°‹æ™‚ï¼Œè«‹å…ˆæ‰¾å›å‰ 6 å€‹æœ€ç›¸é—œçš„å­å‘é‡ (é è¨­é€šå¸¸æ˜¯ 4)
    # retriever.search_kwargs = {"k": 6}

    # 2. è¨­å®š RAG Chain
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    template = """è«‹æ ¹æ“šä»¥ä¸‹æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”å•é¡Œã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±èªªä½ ä¸çŸ¥é“ï¼Œä¸è¦ç·¨é€ ç­”æ¡ˆã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

å•é¡Œï¼š
{question}

å›ç­”ï¼š"""
    prompt = ChatPromptTemplate.from_template(template)

    # å®šç¾©ä¸€å€‹è‡ªå®šç¾©å‡½å¼ä¾†è™•ç†æª¢ç´¢çµæœä¸¦å°å‡ºè³‡è¨Š
    def inspect_and_format_docs(docs):
        print("\n" + "-"*30)
        print(f" ğŸ” [æª¢ç´¢åˆ° {len(docs)} ç­†ç›¸é—œå€å¡Š]")
        
        formatted_contents = []
        for i, doc in enumerate(docs):
            content = doc.page_content.strip()
            # æ ¼å¼åŒ–å…§å®¹é è¦½ï¼šå‰3å­—...å¾Œ3å­—
            preview = f"{content[:3]}...{content[-3:]}" if len(content) > 6 else content
            # å–å¾— Metadata è³‡è¨Š
            source = doc.metadata.get("source", "æœªçŸ¥ä¾†æº")
            chunk_idx = doc.metadata.get("chunk_index", "N/A")
            
            print(f" {i+1}. ä¾†æº: {os.path.basename(source)} (å€å¡Š {chunk_idx}) | é è¦½: {preview}")
            formatted_contents.append(content)
            
        print("-"*30 + "\n")
        return "\n\n".join(formatted_contents)
        
    # ä¿®æ”¹ RAG Chainï¼Œå°‡ format_docs æ”¹æˆæˆ‘å€‘çš„ inspect_and_format_docs
    rag_chain = (
        {
            "context": retriever | inspect_and_format_docs, # é€™è£¡æœƒå…ˆå°å‡ºè³‡è¨Šå†å‚³çµ¦ LLM
            # ä½¿ç”¨ .as_retriever(search_kwargs={"k": 6}) å‹•æ…‹æŒ‡å®šæ•¸é‡
            # "context": retriever.vectorstore.as_retriever(search_kwargs={"k": 6}) | inspect_and_format_docs,
            "question": RunnablePassthrough()
        }
        | prompt
        | llm
        | StrOutputParser()
    )

    # 3. å¤šè¡Œè¼¸å…¥å•ç­”è¿´åœˆ
    print("\n" + "="*50)
    print("æ­¡è¿ä½¿ç”¨ RAG å•ç­”ç³»çµ± (è¼¸å…¥ç©ºç™½è¡ŒçµæŸè¼¸å…¥ä¸¦æäº¤)")
    print("="*50)

    while True:
        print("\nè«‹è¼¸å…¥æ‚¨çš„å•é¡Œ (ç›´æ¥æŒ‰ Enter çµæŸè¼¸å…¥):")
        lines = []
        while True:
            line = input()
            if line == "":
                break
            lines.append(line)
        
        query = "\n".join(lines)
        if not query.strip():
            print("ç¨‹å¼çµæŸã€‚")
            break

        print("\n[æª¢ç´¢ä¸­ä¸¦ç”¢ç”Ÿå›ç­”...]")
        # å–å¾—å›ç­”
        response = rag_chain.invoke(query)
        print(f"\nAI å›ç­”ï¼š\n{response}")

        # 4. è¦–è¦ºåŒ–éƒ¨åˆ†ï¼šæº–å‚™å‘é‡æ•¸æ“š
        print("\n[æ­£åœ¨ç”¢ç”Ÿå‘é‡ç©ºé–“è¦–è¦ºåŒ–åœ–è¡¨...]")
        
        # ç²å–æ‰€æœ‰å­å‘é‡
        all_doc_ids = list(vectorstore.index_to_docstore_id.values())
        vectors = []
        metadata_list = []

        for doc_id in all_doc_ids:
            # å¾ FAISS çš„ docstore å–å¾— child doc
            child_doc = vectorstore.docstore.search(doc_id)
            if child_doc.page_content == "init": continue
            
            # A. å–å¾—å­å‘é‡èˆ‡åŸºæœ¬è³‡è¨Š
            vec = vectorstore.index.reconstruct(all_doc_ids.index(doc_id))
            vectors.append(vec)

            # B. å–å¾—é—œè¯çš„ Parent è³‡è¨Š
            p_hash = child_doc.metadata.get(hash_key_name)
            parent_info = "ç„¡é—œè¯"
            source_file = "æœªçŸ¥"
            
            if p_hash:
                parent_bytes = store.mget([p_hash])[0]
                if parent_bytes:
                    # ä½¿ç”¨ä¹‹å‰å­¸æœƒçš„ langchain_loads
                    p_doc = langchain_loads(parent_bytes.decode('utf-8'))
                    p_content = p_doc.page_content.strip()
                    parent_info = f"{p_content[:3]}...{p_content[-3:]}" if len(p_content) > 6 else p_content
                    source_file = os.path.basename(p_doc.metadata.get("source", "æœªçŸ¥"))

            metadata_list.append({
                "Child_Content": child_doc.page_content[:40] + "...",
                "Type": "Database Vector",
                "Source_File": source_file,
                "Parent_Preview": parent_info
            })

        # åŠ å…¥ç•¶å‰ Query çš„å‘é‡
        query_vec = embeddings.embed_query(query)
        vectors.append(query_vec)
        metadata_list.append({
            "Child_Content": query[:40] + "...",
            "Type": "Your Query",
            "Source_File": "N/A",
            "Parent_Preview": "N/A"
        })

        # PCA é™ç¶­
        vectors_np = np.array(vectors)
        pca = PCA(n_components=2)
        vectors_2d = pca.fit_transform(vectors_np)

        # å»ºç«‹ DataFrame ä¸¦ç¹ªåœ–
        df = pd.DataFrame(vectors_2d, columns=['x', 'y'])
        # å°‡ metadata åˆ—è¡¨è½‰æˆ DataFrame æ¬„ä½
        for key in metadata_list[0].keys():
            df[key] = [m[key] for m in metadata_list]

        fig = px.scatter(
            df, x='x', y='y', color='Type', 
            # é—œéµï¼šåœ¨ hover_data ä¸­åŠ å…¥æ‰€æœ‰æƒ³é¡¯ç¤ºçš„è³‡è¨Š
            hover_data={
                'x': False, 'y': False, # éš±è—åº§æ¨™æ•¸å€¼
                'Type': True,
                'Source_File': True,
                'Child_Content': True,
                'Parent_Preview': True
            },
            title="RAG å‘é‡ç©ºé–“è¦–è¦ºåŒ– (å¸¶æº¯æºè³‡è¨Š)",
            template="plotly_white"
        )
        
        # æ¨™è¨» Query é»çš„å¤§å°ä»¥ä¾¿è­˜åˆ¥
        fig.update_traces(marker=dict(size=10, opacity=0.8))
        fig.show()

if __name__ == "__main__":
    main()